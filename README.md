# ğŸš€ End-to-End Data Engineering Project: dbt, Snowflake & Apache Airflow  

## ğŸ“Œ Overview  
This project is a **complete data engineering pipeline** built with modern tools. It covers **data ingestion, transformation, and orchestration** in a scalable and production-like setup.  

- **dbt (Data Build Tool)** â†’ Data modeling & transformation  
- **Snowflake** â†’ Cloud-based data warehouse  
- **Apache Airflow** â†’ Workflow scheduling & orchestration  
- **Python** â†’ Scripting & automation  

---

## ğŸ› ï¸ Tech Stack  
- **dbt Core** â€“ Transformations, testing, and data modeling  
- **Snowflake** â€“ Cloud-native data warehouse  
- **Apache Airflow** â€“ Orchestration & automation  
- **Python** â€“ Scripting and environment setup  
- **Git/GitHub** â€“ Version control  

---

## ğŸ“‚ Project Structure  
```bash
snowflake_data_project/
â”‚â”€â”€ models/          # dbt models (staging, marts)
â”‚â”€â”€ dags/            # Airflow DAGs (for scheduling dbt runs/tests)
â”‚â”€â”€ logs/            # Airflow logs
â”‚â”€â”€ seeds/           # Sample seed data for dbt
â”‚â”€â”€ macros/          # dbt macros
â”‚â”€â”€ dbt_project.yml  # dbt project config file
â”‚â”€â”€ README.md        # Project documentation
```

---

## âš™ï¸ Setup & Installation  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/your-username/dbt_snowflake_project.git
cd dbt_snowflake_project
```

### 2ï¸âƒ£ Set Up a Virtual Environment  
```bash
# Create venv
python -m venv venv

# Activate (Mac/Linux)
source venv/bin/activate  

# Activate (Windows)
venv\Scripts\activate
```

### 3ï¸âƒ£ Configure dbt Connection to Snowflake  
Update your **`profiles.yml`** located in `~/.dbt/` with your Snowflake credentials:  

```yaml
snowflake_project:
  outputs:
    dev:
      type: snowflake
      account: <your_snowflake_account>
      user: <dbt_user>
      password: <your_password>
      role: ACCOUNTADMIN
      database: finance_db
      warehouse: finance_wh
      schema: raw
  target: dev
```

âš ï¸ **Note:** Never commit credentials. Use environment variables or a `.env` file (add to `.gitignore`).  

---

## â–¶ï¸ Running the Pipeline  

### Run dbt Models  
```bash
dbt run
```

### Run dbt Tests (Validate Data)  
```bash
dbt test
```

### Start Apache Airflow  
```bash
airflow standalone
```
- Access the Airflow UI at **http://localhost:8080**  
- Trigger the `dbt_snowflake_pipeline` DAG  

---

## ğŸ“Š Example Workflow  
âœ… Load raw data â†’  
âœ… Transform with dbt models â†’  
âœ… Validate with dbt tests â†’  
âœ… Orchestrate daily runs via Airflow  


## ğŸ“¬ Contact  
ğŸ‘¤ **Ziad Abdelraouf**  
- ğŸ’¼ [LinkedIn](www.linkedin.com/in/ziad-abdelraouf)  
- ğŸ“‚ [GitHub](https://github.com/Ziadabdelraouf)  
